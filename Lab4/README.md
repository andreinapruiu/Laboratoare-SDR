# ğŸ“˜ Cosine Similarity â€“ Electronics Dataset

## 1. ğŸ“Š Dataset Description

For this project, I used a dataset called **`ElectronicsData.csv`**, which contains **643 electronics products**.
Each row represents a product, with the following relevant columns:

* **Sub Category** â€“ product category (e.g., *Batteries*, *Smart Watches*, *Cameras*)
* **Price** â€“ numeric price
* **Discount** â€“ discount text
* **Rating** â€“ product rating (if available)
* **Title** â€“ product name/title
* **Currency** â€“ currency symbol
* **Feature** â€“ textual description/feature list

To perform text similarity, I used the **Title + Feature** fields concatenated into a single text document per product.

---

## 2. ğŸ§¹ Text Preprocessing

Before computing similarity, I applied **full preprocessing** to clean and normalize the text.
The following steps were performed:

### ğŸ”¤ 1. Lowercasing

All text was converted to lowercase to avoid treating `"Battery"` and `"battery"` as different words.

### ğŸ§¼ 2. Removing HTML + non-alphabetic characters

Although the dataset is clean, the script removes any possible:

* HTML tags
* punctuation
* digits
* special characters

This helps reduce noisy tokens.

### âœ‚ï¸ 3. Tokenization

Each cleaned text was split into individual tokens.

### ğŸ›‘ 4. Stopword Removal

Using **NLTK stopwords**, I removed common English words (e.g., *the*, *and*, *for*, *with*).
These words do not contribute meaningful information for text comparison.

### ğŸ§© 5. Lemmatization

Tokens were lemmatized using `WordNetLemmatizer`, reducing words to their base form:

* *batteries* â†’ *battery*
* *running* â†’ *run*

This ensures that TFâ€“IDF treats different forms of the same word consistently.

### ğŸ§¾ Output of preprocessing

Each product ends up with a clean, lemmatized description containing only meaningful content words.

---

## 3. ğŸ“ TFâ€“IDF Vectorization

To represent each product as a numerical vector, I used **TFâ€“IDF** (`TfidfVectorizer` from scikit-learn).

### Why TFâ€“IDF?

* **TF** â†’ word frequency in that product description
* **IDF** â†’ downweights words that appear in many products
* Produces a sparse, high-dimensional vector for each item

The resulting TFâ€“IDF matrix had shape:

```
(643, N)
```

Where **N** is the size of the vocabulary after preprocessing.

---

## 4. ğŸ”„ Cosine Similarity Matrix

Then, I computed the full **pairwise cosine similarity** matrix:

```
cosine_similarity(tfidf_matrix)
```

Cosine similarity ranges from:

* **1.0** â†’ identical text
* **0.0** â†’ no shared terms

I replaced the diagonal with 0 to avoid comparing each item with itself.

The final matrix (**643 Ã— 643**) was saved as:

ğŸ“„ **`similarity_matrix.csv`**

Rows and columns are indexed by the product titles.

---

## 5. ğŸ” Most Similar Product Pairs

Using the similarity matrix, I extracted:

### âœ” 1. The *most similar pair overall*

(Can include near-duplicates or variants)

### âœ” 2. The *most similar non-identical pair*

(Filtered by excluding values â‰¥ 0.999)

The script prints both pairs, including:

* product titles
* sub-categories
* similarity score

---

## 6. ğŸ§  Examples of Similar Product Pairs (Not Only the Most Similar Ones)

### ğŸ”¹ **Example Pair #1: Smartwatches with Similar Features**

* **Product A:**
  *Apple Watch SE (2nd Generation) (GPS) Sport Loop*

* **Product B:**
  *Apple Watch SE (2nd Generation) (GPS + Cellular) Sport Loop*

**Why they are similar:**

* Both descriptions highlight **health tracking**, **fitness activity**, **sleep monitoring**, and **integration with iPhone**.
* The only feature difference is the presence of **cellular connectivity** in the second model.
* After preprocessing (lemmatization + stopwords removal), both items share a nearly identical set of informative keywords such as:
  *apple, watch, generation, sport, loop, track, workout, active, health, goal, monitor*.

This leads to a cosine similarity score close to **0.99**, which correctly reflects that they are two variants of the same product line.

---

### ğŸ”¹ **Example Pair #2: Home Security Cameras with Overlapping Functionality**

* **Product A:**
  *Ring Indoor Cam (2nd Gen) Plug-In, Compact Indoor Security Camera*

* **Product B:**
  *Blink Mini â€“ Compact Indoor Plug-In HD Security Camera*

**Why they are similar:**

* Both products are **indoor plug-in security cameras**, designed for home monitoring.
* Their feature descriptions emphasize similar capabilities:

  * *HD video*
  * *motion detection*
  * *smart home compatibility*
  * *indoor surveillance*
* After cleaning the text, both descriptions heavily focus on terms like:
  *indoor, camera, plug, security, motion, detection, video, home, compact*
* They belong to the same functional category even though they are different brands (Ring vs. Blink).

Their cosine similarity score is high because the TFâ€“IDF representation captures that they describe almost identical **use cases** and **attributes**.

---

## 7. ğŸ“ Output Files

The project produces:

### **1.** `similarity_matrix.csv`

A full cosine similarity matrix between all product descriptions.

### **2.** Console output:

* Most similar pair (including duplicates)
* Most similar non-identical pair
* Their similarity scores
* Product titles and sub-categories

---

## 8. ğŸ§© Technologies Used

* **Python 3**
* **Pandas** â€“ dataset loading
* **NLTK** â€“ tokenization, stopwords, lemmatization
* **scikit-learn** â€“ TFâ€“IDF, cosine similarity
* **NumPy** â€“ matrix operations